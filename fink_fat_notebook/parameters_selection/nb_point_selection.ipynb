{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark-3/python/pyspark/sql/pandas/functions.py:392: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"in the future releases. See SPARK-28264 for more details.\", UserWarning)\n",
      "/opt/spark-3/python/pyspark/sql/pandas/functions.py:392: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"in the future releases. See SPARK-28264 for more details.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from fink_filters.classification import extract_fink_classification\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "import pandas as pd\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import time as t\n",
    "\n",
    "from dateutil import rrule\n",
    "from datetime import datetime, timedelta, date\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col, create_map, lit\n",
    "from itertools import chain\n",
    "\n",
    "from fink_fat.orbit_fitting.orbfit_cluster import orbit_wrapper\n",
    "\n",
    "\n",
    "import exploring_script as es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ast_traj(df):\n",
    "    class_df = df.withColumn(\n",
    "        \"class\",\n",
    "        extract_fink_classification(\n",
    "            df[\"cdsxmatch\"],\n",
    "            df[\"roid\"], \n",
    "            df[\"mulens\"],\n",
    "            df[\"snn_snia_vs_nonia\"], \n",
    "            df[\"snn_sn_vs_all\"], \n",
    "            df[\"rf_snia_vs_nonia\"],\n",
    "            df[\"candidate.ndethist\"], \n",
    "            df[\"candidate.drb\"], \n",
    "            df[\"candidate.classtar\"], \n",
    "            df[\"candidate.jd\"], \n",
    "            df[\"candidate.jdstarthist\"], \n",
    "            df[\"rf_kn_vs_nonkn\"], \n",
    "            df[\"tracklet\"]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    sso_class = class_df.filter(class_df[\"class\"] == \"Solar System MPC\")\n",
    "    w = Window.partitionBy('candidate.ssnamenr')\n",
    "    sso_class = sso_class.select(\n",
    "        sso_class[\"objectId\"],\n",
    "        sso_class[\"candidate.candid\"],\n",
    "        sso_class[\"candidate.ra\"],\n",
    "        sso_class[\"candidate.dec\"],\n",
    "        sso_class[\"candidate.jd\"],\n",
    "        sso_class[\"candidate.nid\"],\n",
    "        sso_class[\"candidate.fid\"],\n",
    "        sso_class[\"candidate.ssnamenr\"],\n",
    "        sso_class[\"candidate.ssdistnr\"],\n",
    "        sso_class[\"candidate.magpsf\"],\n",
    "        sso_class[\"candidate.sigmapsf\"],\n",
    "        sso_class[\"candidate.magnr\"],\n",
    "        sso_class[\"candidate.sigmagnr\"],\n",
    "        sso_class[\"candidate.magzpsci\"],\n",
    "        sso_class[\"candidate.isdiffpos\"],\n",
    "        F.count('candidate.ssnamenr').over(w).alias('nb_detection')\n",
    "    )\n",
    "    return sso_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"/user/julien.peloton/archive/science/year=2022/month=11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_sso = get_ast_traj(df).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_tw(x, tw_traj):\n",
    "    jd = x[\"jd\"]\n",
    "    \n",
    "    diff_jd = np.diff(jd)\n",
    "    \n",
    "    return np.all(diff_jd < tw_traj)\n",
    "\n",
    "def get_traj_limit(df, tw, limit):\n",
    "    list_sso_gb = df.sort_values(\"jd\").groupby(\"ssnamenr\").agg(list)\n",
    "    \n",
    "    list_limit_sso = list_sso_gb[list_sso_gb.apply(lambda x: is_in_tw(x, tw), axis=1)]\n",
    "    \n",
    "    for df_col in list_limit_sso.columns:\n",
    "        list_limit_sso[df_col] = list_limit_sso[df_col].apply(lambda x: x[:limit])\n",
    "    \n",
    "    list_limit_sso = list_limit_sso.reset_index()\n",
    "    all_ssnamenr = {\n",
    "        sso_name:id_traj \n",
    "        for sso_name, id_traj in zip(list_limit_sso[\"ssnamenr\"], np.arange(len(list_limit_sso)))\n",
    "    }\n",
    "    list_limit_sso[\"trajectory_id\"] = list_limit_sso[\"ssnamenr\"].map(all_ssnamenr)\n",
    "    \n",
    "    return list_limit_sso.set_index([\"ssnamenr\", \"trajectory_id\"]).apply(pd.Series.explode).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TW = 15\n",
    "\n",
    "nb_traj_limit = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_orbit(nb_point):\n",
    "    confirmed_point_limit = confirmed_sso[confirmed_sso[\"nb_detection\"] >= nb_point]\n",
    "    prep_to_orbit = get_traj_limit(confirmed_sso, TW, nb_point)\n",
    "    \n",
    "    prep_to_orbit.to_parquet(\"res_orbit_nb_point/{}_point_traj.parquet\".format(nb_point))\n",
    "    \n",
    "    # transform the local pandas dataframe into a spark dataframe\n",
    "    sparkDF = spark.createDataFrame(prep_to_orbit)\n",
    "\n",
    "    spark_gb = (\n",
    "        sparkDF.groupby(\"trajectory_id\")\n",
    "        .agg(\n",
    "            F.sort_array(\n",
    "                F.collect_list(F.struct(\"jd\", \"ra\", \"dec\", \"fid\", \"magpsf\"))\n",
    "            ).alias(\"collected_list\")\n",
    "        )\n",
    "        .withColumn(\"ra\", F.col(\"collected_list.ra\"))\n",
    "        .withColumn(\"dec\", F.col(\"collected_list.dec\"))\n",
    "        .withColumn(\"fid\", F.col(\"collected_list.fid\"))\n",
    "        .withColumn(\"magpsf\", F.col(\"collected_list.magpsf\"))\n",
    "        .withColumn(\"jd\", F.col(\"collected_list.jd\"))\n",
    "        .drop(\"collected_list\")\n",
    "    )\n",
    "    \n",
    "    spark_gb = spark_gb.limit(nb_traj_limit)\n",
    "\n",
    "    max_core = int(dict(spark.sparkContext.getConf().getAll())[\"spark.cores.max\"])\n",
    "    spark_gb = spark_gb.repartition(1 if nb_traj_limit // max_core == 0 else nb_traj_limit // max_core)\n",
    "\n",
    "    spark_column = spark_gb.withColumn(\n",
    "        \"orbital_elements\",\n",
    "        orbit_wrapper(\n",
    "            spark_gb.ra,\n",
    "            spark_gb.dec,\n",
    "            spark_gb.magpsf,\n",
    "            spark_gb.fid,\n",
    "            spark_gb.jd,\n",
    "            spark_gb.trajectory_id,\n",
    "            \"/tmp/ramdisk/roman\",\n",
    "            30,\n",
    "            20,\n",
    "            None,\n",
    "            verbose=3,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    res_orbit = spark_column.toPandas()\n",
    "\n",
    "    orbital_columns = [\n",
    "        \"ref_epoch\",\n",
    "        \"a\",\n",
    "        \"e\",\n",
    "        \"i\",\n",
    "        \"long. node\",\n",
    "        \"arg. peric\",\n",
    "        \"mean anomaly\",\n",
    "        \"rms_a\",\n",
    "        \"rms_e\",\n",
    "        \"rms_i\",\n",
    "        \"rms_long. node\",\n",
    "        \"rms_arg. peric\",\n",
    "        \"rms_mean anomaly\",\n",
    "        \"chi_reduced\",\n",
    "    ]\n",
    "\n",
    "    split_df = pd.DataFrame(\n",
    "        res_orbit[\"orbital_elements\"].tolist(), columns=orbital_columns\n",
    "    )\n",
    "    orbit_results = pd.concat([res_orbit[\"trajectory_id\"], split_df], axis=1)\n",
    "    orbit_results.to_parquet(\"res_orbit_nb_point/{}_point_orbit.parquet\".format(nb_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 point / elapsed time: 389.13769006729126\n",
      "4 point / elapsed time: 391.4380407333374\n",
      "5 point / elapsed time: 399.9971191883087\n",
      "6 point / elapsed time: 407.40319657325745\n",
      "7 point / elapsed time: 412.00954723358154\n",
      "8 point / elapsed time: 411.9934003353119\n",
      "9 point / elapsed time: 414.9642653465271\n",
      "10 point / elapsed time: 416.444669008255\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(3, 11):\n",
    "    t_before = t.time()\n",
    "    compute_orbit(i)\n",
    "    print(\"{} point / elapsed time: {}\".format(i, t.time() - t_before))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
